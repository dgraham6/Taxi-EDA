{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c85287-8cd3-49f1-8162-eff4571da8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "import googlemaps\n",
    "import time\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from tqdm import tqdm\n",
    "import mpld3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c69363c-ad2b-4645-b3e2-0d2c293497fb",
   "metadata": {},
   "source": [
    "Important links\n",
    "Simlar Competition\n",
    "https://www.kaggle.com/code/headsortails/nyc-taxi-eda-update-the-fast-the-curious/report#fastest-routes\n",
    "https://www.kaggle.com/code/karelrv/nyct-from-a-to-z-with-xgboost-tutorial\n",
    "\n",
    "Where I got the taxi data \n",
    "https://catalog.data.gov/dataset/taxi-trips-in-2024\n",
    "\n",
    "Where I got the weather data\n",
    "https://www.visualcrossing.com/weather/weather-data-services\n",
    "\n",
    "Quotes\n",
    "https://kandmcollision.com/car-mean-time/#:~:text=What%20you%20don't%20know,moon%20and%20back%203%20times!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5517da-d06b-4ef0-bf17-eb8304bd319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = pd.read_csv('curve.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4795dbc-4b01-49f6-98d2-f961068c192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data = taxi.select_dtypes(include=['number'])\n",
    "numerical_data = numerical_data.drop(['ORIGIN_BLOCK_LATITUDE', \n",
    "                                     'ORIGIN_BLOCK_LONGITUDE','DESTINATION_BLOCK_LAT', 'DESTINATION_BLOCK_LONG', 'Duration(m)',\n",
    "      'duration', 'distance', 'precipprob', 'snow', 'snowdepth', 'MILEAGE', 'steps', 'Day_Monday',\n",
    "       'Day_Saturday', 'Day_Sunday', 'Day_Thursday', 'Day_Tuesday', 'snowfall', 'precipitation',\n",
    "       'Day_Wednesday'\n",
    "       ], axis=1)\n",
    "correlation_matrix = numerical_data.corr()\n",
    "\n",
    "# Create an interactive heatmap with Plotly\n",
    "fig = px.imshow(\n",
    "    correlation_matrix,\n",
    "    text_auto=\".2f\",\n",
    "    title=\"Correlation Matrix Heatmap\"\n",
    ")\n",
    "# Save as an HTML file\n",
    "fig.write_html(\"matrix.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48065dc0-ce9b-4983-b176-3cc5e9424c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_center(X):\n",
    "    return X.assign(\n",
    "        CenterLat=(X[\"ORIGIN_BLOCK_LATITUDE\"] + X[\"DESTINATION_BLOCK_LAT\"]) / 2,\n",
    "        CenterLong=(X[\"ORIGIN_BLOCK_LONGITUDE\"] + X[\"DESTINATION_BLOCK_LONG\"]) / 2\n",
    "    )\n",
    "def compute_direction(X):\n",
    "    return X.assign(\n",
    "        Direction=X.apply(\n",
    "            lambda row: (\n",
    "                'NorthEast' if row['DESTINATION_BLOCK_LAT'] > row['ORIGIN_BLOCK_LATITUDE'] and row['DESTINATION_BLOCK_LONG'] > row['ORIGIN_BLOCK_LONGITUDE'] else\n",
    "                'NorthWest' if row['DESTINATION_BLOCK_LAT'] > row['ORIGIN_BLOCK_LATITUDE'] and row['DESTINATION_BLOCK_LONG'] < row['ORIGIN_BLOCK_LONGITUDE'] else\n",
    "                'SouthEast' if row['DESTINATION_BLOCK_LAT'] <= row['ORIGIN_BLOCK_LATITUDE'] and row['DESTINATION_BLOCK_LONG'] >= row['ORIGIN_BLOCK_LONGITUDE'] else\n",
    "                'SouthWest'\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "    )\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 3959\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    return R * c\n",
    "    \n",
    "def compute_distance(X):\n",
    "    return X.assign(\n",
    "        Distance=(haversine(X[\"ORIGIN_BLOCK_LATITUDE\"], X[\"ORIGIN_BLOCK_LONGITUDE\"],X[\"DESTINATION_BLOCK_LAT\"], X[\"DESTINATION_BLOCK_LONG\"]))\n",
    "    )\n",
    "\n",
    "busy_roads = [\n",
    "    'East Capitol Street Southeast',\n",
    "    'New York Avenue Northeast',\n",
    "    'Florida Avenue Northeast',\n",
    "    '12'\n",
    "]\n",
    "\n",
    "def check_roads(x):\n",
    "    for road in x:\n",
    "        if str(road) in str(busy_roads):\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0cac5e-ddb1-4a3d-a21a-d6bcf15ec5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# List of busy roads\n",
    "busy_roads = [\n",
    "    'East Capitol Street Southeast',\n",
    "    'New York Avenue Northeast',\n",
    "    'Florida Avenue Northeast',\n",
    "    'H St ',\n",
    "    'Minnesota',\n",
    "    'MLK',\n",
    "    'Grant Street'\n",
    "]\n",
    "\n",
    "# Function to check if any road in the list is a busy road\n",
    "def check_roads(road_list):\n",
    "    for road in road_list:\n",
    "        for busy_road in busy_roads:\n",
    "            if busy_road in road:  # Substring match\n",
    "                return 1\n",
    "    return 0\n",
    "\n",
    "# Apply the function to the 'roads' column\n",
    "taxi['Congested_roads'] = taxi['roads'].apply(check_roads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3c6c2e-6fa4-4c0f-9dd1-8532277308ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "plt.hexbin(taxi['steps(OSRM)'], taxi['DURATION'], gridsize=30, cmap='Blues')\n",
    "plt.colorbar(label='Density')\n",
    "plt.title(\"Steps vs Duration(s)\", fontsize=16)\n",
    "plt.xlabel(\"Steps\", fontsize=14)\n",
    "plt.ylabel(\"Duration(s)\", fontsize=14)\n",
    "plt.xlim(0, 20)  # Adjusting x-axis range\n",
    "plt.ylim(0,2500)\n",
    "plt.grid(True)\n",
    "plt.savefig('hexa.jpeg', format='jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9572411-67ce-45d6-a217-41747b8aadb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.kdeplot(\n",
    "    x=taxi['steps(OSRM)'], \n",
    "    y=taxi['DURATION'], \n",
    "    cmap=\"Blues\", fill=True, thresh=0, levels=100\n",
    ")\n",
    "plt.scatter(taxi['steps(OSRM)'], taxi['DURATION'], s=10, color=\"red\", alpha=0.5)\n",
    "plt.title(\"KDE Plot of steps(OSRM) vs DURATION\", fontsize=16)\n",
    "plt.xlabel(\"steps(OSRM)\", fontsize=14)\n",
    "plt.ylabel(\"DURATION\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a61e16a-614e-49b4-b1ec-2dde9ce6b699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"xgboost\")\n",
    "import logging\n",
    "\n",
    "# Suppress specific LightGBM warnings\n",
    "logging.getLogger(\"lightgbm\").setLevel(logging.ERROR)\n",
    "# Define Preprocessors\n",
    "num_features = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "cat_features = X_train.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# Preprocessors\n",
    "num_preprocessor = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_preprocessor = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", max_categories=20))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_preprocessor, num_features),\n",
    "    (\"cat\", cat_preprocessor, cat_features)\n",
    "])\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    'Ridge': Pipeline([(\"preprocessor\", preprocessor), (\"model\", Ridge())]),\n",
    "    #'Lasso': Pipeline([(\"preprocessor\", preprocessor), (\"model\", Lasso(max_iter=50000))]),\n",
    "  #  'ElasticNet': Pipeline([(\"preprocessor\", preprocessor), (\"model\", ElasticNet(max_iter=50000))]),\n",
    "  #  'SVR': Pipeline([(\"preprocessor\", preprocessor), (\"model\", SVR())]),\n",
    "   # 'RandomForest': Pipeline([(\"preprocessor\", preprocessor), (\"model\", RandomForestRegressor(\n",
    " #      n_estimators=300, max_depth=15, min_samples_split=5, min_samples_leaf=2\n",
    "  #  ))]),\n",
    " #   'GradientBoosting': Pipeline([(\"preprocessor\", preprocessor), (\"model\", GradientBoostingRegressor())]),\n",
    " #   'XGBoost': Pipeline([(\"preprocessor\", preprocessor), (\"model\", XGBRegressor(\n",
    "#        n_estimators=1000, eval_metric=\"rmse\", use_label_encoder=False\n",
    "#    ))]),\n",
    " #   'LightGBM': Pipeline([(\"preprocessor\", preprocessor), (\"model\", LGBMRegressor(\n",
    " #       feature_fraction=0.8,\n",
    " #       min_data_in_leaf=10,\n",
    " #       force_row_wise=True,\n",
    " #       verbose=-1\n",
    " #   ))]),\n",
    " #   'CatBoost': Pipeline([(\"preprocessor\", preprocessor), (\"model\", CatBoostRegressor(\n",
    " #       verbose=0, early_stopping_rounds=10\n",
    " #   ))]),\n",
    "}\n",
    "\n",
    "# Parameter Grids\n",
    "param_grids = {\n",
    "    'Ridge': {'model__alpha': [50,100,200]},\n",
    "   # 'Lasso': {'model__alpha': [0.001, 0.01,0.1]},\n",
    "   # 'ElasticNet': {'model__alpha': [0.0001, 0.001], 'model__l1_ratio': [0.5, 0.9]},\n",
    "   # 'SVR': {'model__C': [0.1, 1, 10], 'model__epsilon': [0.1]},\n",
    "   # 'RandomForest': {\n",
    " #      'model__n_estimators': [100, 200],\n",
    "   #     'model__max_depth': [10, 2],\n",
    "   #     'model__min_samples_split': [2, 5, 10],\n",
    "   #     'model__min_samples_leaf': [1,4],\n",
    "     #   'model__max_features': ['sqrt', 'log2', None],\n",
    "    #    'model__bootstrap': [True, False]\n",
    "  #  }\n",
    "#    'GradientBoosting': {'model__n_estimators': [100, 150, 200], 'model__learning_rate': [0.05, 0.1]},\n",
    "#    'XGBoost': {\n",
    "#        'model__n_estimators': [50, 100],\n",
    "#        'model__learning_rate': [0.01, 0.1],\n",
    "#        'model__max_depth': [3, 6],\n",
    "#        'model__subsample': [0.8, 0.3],\n",
    "#    },\n",
    "#    'LightGBM': {\n",
    "#        'model__n_estimators': [100, 200],\n",
    "##        'model__learning_rate': [0.05, 0.1],\n",
    " #       'model__feature_fraction': [0.8, 1.0],\n",
    " #   },\n",
    " #   'CatBoost': {\n",
    " #   'model__iterations': [500],\n",
    " #   'model__depth': [6],\n",
    " #   'model__learning_rate': [0.01],\n",
    " #   'model__subsample': [0.8, 0.3],\n",
    " #   'model__bootstrap_type': ['Bayesian', 'Bernoulli']\n",
    " #   }\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "\n",
    "for model_name, model in tqdm(models.items(), desc=\"Processing Models\"):\n",
    "    grid = GridSearchCV(model, param_grids[model_name], scoring=scorer, cv=5)\n",
    "    grid.fit(X_train, y_train_log)\n",
    "    results[model_name] = {\n",
    "        'best_score': np.sqrt(-grid.best_score_),\n",
    "        'best_params': grid.best_params_,\n",
    "    }\n",
    "\n",
    "resultssorted_results = sorted(results.items(), key=lambda x: x[1]['best_score'])\n",
    "for model_name, result in sorted_results:\n",
    "    print(f\"{model_name}: RMSLE = {result['best_score']:.4f}, Best Params ="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
